{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ewC1YOQgtbBj",
    "outputId": "dbecb21e-774c-4d78-aa65-d8f78cd8010d"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MeXnIUcEthYO"
   },
   "outputs": [],
   "source": [
    "! pip install transformers -q\n",
    "! pip install dgl-cu100 -q\n",
    "! pip install word2vec -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IOFhm9xStmQV"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import *\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.utils.extmath import softmax\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import classification_report, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-JsXpQNttsRg",
    "outputId": "57293a3b-e299-4e6e-8a12-7107fd946919"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import transformers\n",
    "from transformers import AdamW\n",
    "import dgl\n",
    "import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vTdjnvlytwNV"
   },
   "outputs": [],
   "source": [
    "def seed_all(seed = 42):\n",
    "  \"\"\"\n",
    "  Fix seed for reproducibility\n",
    "  \"\"\"\n",
    "  # python RNG\n",
    "  import random\n",
    "  random.seed(seed)\n",
    "\n",
    "  # pytorch RNGs\n",
    "  import torch\n",
    "  torch.manual_seed(seed)\n",
    "  torch.backends.cudnn.deterministic = True\n",
    "  if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "  # numpy RNG\n",
    "  import numpy as np\n",
    "  np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mMHkNSVGumlX"
   },
   "outputs": [],
   "source": [
    "class config:\n",
    "  ADD_SUBJ = False\n",
    "  SAVE_DIR = 'bert_gcn'\n",
    "  KFOLD = 3\n",
    "  MAX_LEN = 96\n",
    "  SEED = 42\n",
    "  MODEL = 'bert-base-cased'\n",
    "  TOKENIZER = transformers.BertTokenizer.from_pretrained(MODEL)\n",
    "  EPOCHS = 1\n",
    "  TRAIN_BATCH_SIZE = 32\n",
    "  VALID_BATCH_SIZE = 32\n",
    "  TRAIN_FILE = 'task1_train.csv'\n",
    "  VAL_FILE =  'task1_dev.csv'\n",
    "  TEST_FILE = 'task1_test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qAbVky6Kt4IV"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/gdrive/My Drive/DEFINITION EXTRACTION/DEFT_Updated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j-_-jJZ9zYYN"
   },
   "outputs": [],
   "source": [
    "class AverageMeter:\n",
    "    \"\"\"\n",
    "    Computes and stores the average and current value\n",
    "    Source : https://www.kaggle.com/abhishek/bert-base-uncased-using-pytorch/\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_9uPTGIo1ihU"
   },
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Early stopping utility\n",
    "    Source : https://www.kaggle.com/abhishek/bert-base-uncased-using-pytorch/\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, patience=7, mode=\"max\", delta=0.001):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.mode = mode\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.delta = delta\n",
    "        if self.mode == \"min\":\n",
    "            self.val_score = np.Inf\n",
    "        else:\n",
    "            self.val_score = -np.Inf\n",
    "\n",
    "    def __call__(self, epoch_score, model, model_path):\n",
    "        if self.mode == \"min\":\n",
    "            score = -1.0 * epoch_score\n",
    "        else:\n",
    "            score = np.copy(epoch_score)\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(epoch_score, model, model_path)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print('EarlyStopping counter: {} out of {}'.format(self.counter, self.patience))\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(epoch_score, model, model_path)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, epoch_score, model, model_path):\n",
    "        if epoch_score not in [-np.inf, np.inf, -np.nan, np.nan]:\n",
    "            print('Validation score improved ({} --> {}). Saving model!'.format(self.val_score, epoch_score))\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "        self.val_score = epoch_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IoZ1L2smuMDd"
   },
   "outputs": [],
   "source": [
    "def process_data(text, subject, tokenizer, max_len, label):\n",
    "\n",
    "  ## remove initial numbers\n",
    "  text = re.findall('^\\s*\\d*\\s*\\.?\\s*(.*)', text)[0]  \n",
    "\n",
    "  ## add subject\n",
    "  if config.ADD_SUBJ:\n",
    "    text = subject + ' ' + text\n",
    "  \n",
    "  token_ids = tokenizer.encode(text, add_special_tokens=True)\n",
    "  mask = [1] * len(token_ids)\n",
    "\n",
    "  padding = max_len - len(token_ids)\n",
    "  \n",
    "  if padding>=0:\n",
    "    token_ids = token_ids + ([0] * padding)\n",
    "    mask = mask + ([0] * padding)\n",
    "  else:\n",
    "    token_ids = token_ids[0:max_len]\n",
    "    mask = mask[0:max_len]\n",
    "\n",
    "  return {'text':text,\n",
    "          'subject':subject,\n",
    "          'ids':token_ids,\n",
    "          'mask':mask,\n",
    "          'label':label\n",
    "          }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vOSSfTVYBooJ"
   },
   "source": [
    "## The [official implementation](https://github.com/HuangLianzhe/TextLevelGCN) of Text Level GCN has been used to build the BERT GCN Joint Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XUhLupPgwL1M"
   },
   "outputs": [],
   "source": [
    "class DEFTDataset:\n",
    "    def __init__(self, text, subject, label, vocab=None):\n",
    "        self.text = text\n",
    "        self.subject = subject\n",
    "        self.label = label\n",
    "        self.tokenizer = config.TOKENIZER\n",
    "        self.max_len = config.MAX_LEN\n",
    "\n",
    "        if vocab is None:\n",
    "          self.vocab = []\n",
    "          try:\n",
    "              self.get_vocab(vocab)\n",
    "              print(f'Vocab file found')\n",
    "          except:\n",
    "              self.build_vocab(self.text, min_count=5)\n",
    "        else:\n",
    "            self.vocab = vocab\n",
    "\n",
    "        self.d = dict(zip(self.vocab, range(len(self.vocab))))\n",
    "        self.rev_d = { v:k for k,v in self.d.items()}\n",
    "\n",
    "\n",
    "    def word2id(self, word):\n",
    "        try:\n",
    "            result = self.d[word]\n",
    "        except KeyError:\n",
    "            result = self.d['UNK']\n",
    "        return result\n",
    "\n",
    "    def get_vocab(self, filename):\n",
    "        with open(filename) as f:\n",
    "            vocab = f.read()\n",
    "            self.vocab = vocab.split('\\n')\n",
    "\n",
    "    def build_vocab(self, content, min_count=10):\n",
    "        vocab = []\n",
    "        for c in content:\n",
    "            words = c.split(' ')\n",
    "            for word in words:\n",
    "                if word not in vocab:\n",
    "                    vocab.append(word)\n",
    "        freq = dict(zip(vocab, [0 for i in range(len(vocab))]))\n",
    "        for c in content:\n",
    "            words = c.split(' ')\n",
    "            for word in words:\n",
    "                freq[word] += 1\n",
    "        results = []\n",
    "        for word in freq.keys():\n",
    "            if freq[word] < min_count:\n",
    "                continue\n",
    "            else:\n",
    "                results.append(word)\n",
    "        results.insert(0, 'UNK')\n",
    "        with open('vocab.txt', 'w') as f:\n",
    "            f.write('\\n'.join(results))\n",
    "        self.vocab = results\n",
    "\n",
    "\n",
    "    def get_gcn_data(self, text, max_len):\n",
    "      seq = list(map(lambda x: self.word2id(x), text.split(' ')))\n",
    "      length = len(seq)\n",
    "\n",
    "      padding = max_len - length\n",
    "      if padding>=0:\n",
    "        seq = seq + [0] * padding\n",
    "      else:\n",
    "        seq = seq[0:max_len]\n",
    "\n",
    "      return {'ids': seq, \n",
    "              'length': length,\n",
    "              }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        data = process_data(\n",
    "            self.text[item], \n",
    "            self.subject[item], \n",
    "            self.tokenizer,\n",
    "            self.max_len,\n",
    "            self.label[item],\n",
    "        )\n",
    "\n",
    "        gcn_data = self.get_gcn_data(data['text'], self.max_len)\n",
    "\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(data[\"ids\"], dtype=torch.long),\n",
    "            'mask': torch.tensor(data[\"mask\"], dtype=torch.long),\n",
    "            'gcn_ids' : torch.tensor(gcn_data[\"ids\"], dtype=torch.long),\n",
    "            'gcn_length' : torch.tensor(gcn_data[\"length\"], dtype=torch.long),\n",
    "            'text': data['text'],\n",
    "            'subject': data['subject'],\n",
    "            'label': data['label'],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H8cxhzM1J39Y"
   },
   "outputs": [],
   "source": [
    "def cal_PMI(helper, window_size=20):\n",
    "    content = []\n",
    "    for d in helper:\n",
    "      content.append(d['text'])\n",
    "\n",
    "    # co-occurence matrix\n",
    "    pair_count_matrix = np.zeros((len(helper.vocab), len(helper.vocab)), dtype=int)\n",
    "    # frequency of words\n",
    "    word_count =np.zeros(len(helper.vocab), dtype=int)\n",
    "\n",
    "    for sentence in tqdm(content):\n",
    "        sentence = sentence.split(' ')\n",
    "        for i, word in enumerate(sentence):\n",
    "            try:\n",
    "                word_count[helper.d[word]] += 1\n",
    "            except KeyError:\n",
    "                continue\n",
    "            start_index = max(0, i - window_size)\n",
    "            end_index = min(len(sentence), i + window_size)\n",
    "            # iterating over n-gram neighbourhood\n",
    "            for j in range(start_index, end_index):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                else:\n",
    "                    target_word = sentence[j]\n",
    "                    try:\n",
    "                        pair_count_matrix[helper.d[word], helper.d[target_word]] += 1\n",
    "                    except KeyError:\n",
    "                        continue\n",
    "\n",
    "    total_count = np.sum(word_count)\n",
    "    word_count = word_count / total_count\n",
    "    pair_count_matrix = pair_count_matrix / total_count\n",
    "\n",
    "    pmi_matrix = np.zeros((len(helper.vocab), len(helper.vocab)), dtype=float)\n",
    "    for i in range(len(helper.vocab)):\n",
    "        for j in range(len(helper.vocab)):\n",
    "            pmi_matrix[i, j] = np.log(\n",
    "                pair_count_matrix[i, j] / (word_count[i] * word_count[j])\n",
    "            )\n",
    "\n",
    "    pmi_matrix = np.nan_to_num(pmi_matrix)\n",
    "    pmi_matrix = np.maximum(pmi_matrix, 0.0)\n",
    "\n",
    "    # map edge between two words to edge-id\n",
    "    edges_weights = [0.0]\n",
    "    count = 1\n",
    "    edges_mappings = np.zeros((len(helper.vocab), len(helper.vocab)), dtype=int)\n",
    "    for i in range(len(helper.vocab)):\n",
    "        for j in range(len(helper.vocab)):\n",
    "            if pmi_matrix[i, j] != 0:\n",
    "                edges_weights.append(pmi_matrix[i, j])\n",
    "                edges_mappings[i, j] = count\n",
    "                count += 1\n",
    "\n",
    "    edges_weights = np.array(edges_weights)\n",
    "    edges_weights = edges_weights.reshape(-1, 1)\n",
    "    edges_weights = torch.Tensor(edges_weights)\n",
    "\n",
    "    return edges_weights, edges_mappings, count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "msB4_TFGJ5eu"
   },
   "outputs": [],
   "source": [
    "class GCNModel(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 hidden_size_node,\n",
    "                 vocab,\n",
    "                 n_gram,\n",
    "                 edges_num,\n",
    "                 edges_matrix,\n",
    "                 max_length=350,\n",
    "                 trainable_edges=True,\n",
    "                 pmi=None,\n",
    "                 cuda=True\n",
    "                 ):\n",
    "        super(GCNModel, self).__init__()\n",
    "\n",
    "        self.is_cuda = cuda\n",
    "        self.vocab = vocab\n",
    "        \n",
    "        self.seq_edge_w = torch.nn.Embedding(edges_num, 1)\n",
    "        self.node_hidden = torch.nn.Embedding(len(vocab), hidden_size_node)\n",
    "        \n",
    "        self.seq_edge_w = torch.nn.Embedding.from_pretrained(pmi, freeze=True)\n",
    "            \n",
    "        self.edges_num = edges_num\n",
    "        if trainable_edges:\n",
    "            self.seq_edge_w = torch.nn.Embedding.from_pretrained(torch.ones(edges_num, 1), freeze=False)\n",
    "        else:\n",
    "            self.seq_edge_w = torch.nn.Embedding.from_pretrained(pmi, freeze=True)\n",
    "\n",
    "        self.hidden_size_node = hidden_size_node\n",
    "        self.node_hidden.weight.data.copy_(torch.tensor(self.load_word2vec('glove.6B.200d.vec.txt')))\n",
    "        self.node_hidden.weight.requires_grad = True\n",
    "\n",
    "        self.len_vocab = len(vocab)\n",
    "        self.ngram = n_gram\n",
    "        self.d = dict(zip(self.vocab, range(len(self.vocab))))\n",
    "        self.max_length = max_length\n",
    "        self.edges_matrix = edges_matrix\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "\n",
    "        \n",
    "    def word2id(self, word):\n",
    "        try:\n",
    "            result = self.d[word]\n",
    "        except KeyError:\n",
    "            result = self.d['UNK']\n",
    "        return result\n",
    "\n",
    "    def load_word2vec(self, word2vec_file):\n",
    "        model = word2vec.load(word2vec_file)\n",
    "        embedding_matrix = []\n",
    "        for word in self.vocab:\n",
    "            try:\n",
    "                embedding_matrix.append(model[word])\n",
    "            except KeyError:\n",
    "                embedding_matrix.append(model['the'])\n",
    "        embedding_matrix = np.array(embedding_matrix)\n",
    "        return embedding_matrix\n",
    "\n",
    "    \n",
    "\n",
    "    def add_seq_edges(self, doc_ids: list, old_to_new: dict):\n",
    "        '''\n",
    "          doc_ids : list of ids of words in a sentence\n",
    "          old_to_new : the ids of the words are global; it is mapping of these ids to local ids(text-level graph)\n",
    "\n",
    "          The function returns the edge list w.r.t. text-level graph and the corresponding global edge ids\n",
    "\n",
    "        '''\n",
    "\n",
    "        edges = []\n",
    "        old_edge_id = []\n",
    "        for index, src_word_old in enumerate(doc_ids):\n",
    "            src = old_to_new[int(src_word_old.item())]\n",
    "            for i in range(max(0, index - self.ngram), min(index + self.ngram + 1, len(doc_ids))):\n",
    "                dst_word_old = doc_ids[i].item()\n",
    "                dst = old_to_new[dst_word_old]\n",
    "\n",
    "                # - first connect the new sub_graph\n",
    "                edges.append([src, dst])\n",
    "                # - then get the hidden from parent_graph\n",
    "                old_edge_id.append(self.edges_matrix[src_word_old, dst_word_old])\n",
    "\n",
    "            # self circle\n",
    "            edges.append([src, src])\n",
    "            old_edge_id.append(self.edges_matrix[src_word_old, src_word_old])\n",
    "        return edges, old_edge_id\n",
    "\n",
    "    def seq_to_graph(self, doc_ids: list, doc_length) -> dgl.DGLGraph():\n",
    "        '''\n",
    "            doc_ids : global ids of words in a sentence\n",
    "            doc_length : the actual length of the sentence without padding\n",
    "\n",
    "            The function returns the text-level graph for the sentence\n",
    "\n",
    "        '''\n",
    "\n",
    "        doc_ids = doc_ids[0:doc_length]\n",
    "        if len(doc_ids) > self.max_length:\n",
    "            doc_ids = doc_ids[:self.max_length]\n",
    "\n",
    "\n",
    "        local_vocab = set(doc_ids)\n",
    "        old_to_new = {}\n",
    "        # mapping of words global ids to local ids\n",
    "        for i,j in enumerate(local_vocab):\n",
    "          old_to_new[j.item()] = i\n",
    "        \n",
    "\n",
    "        if self.is_cuda:\n",
    "            local_vocab = torch.tensor(list(local_vocab)).cuda()\n",
    "        else:\n",
    "            local_vocab = torch.tensor(list(local_vocab))\n",
    "\n",
    "        # create dgl graph\n",
    "        sub_graph = dgl.DGLGraph()\n",
    "\n",
    "        sub_graph.add_nodes(len(local_vocab))\n",
    "        local_node_hidden = self.node_hidden(local_vocab)\n",
    "\n",
    "        sub_graph.ndata['h'] = local_node_hidden\n",
    "\n",
    "        seq_edges, seq_old_edges_id = self.add_seq_edges(doc_ids, old_to_new)\n",
    "\n",
    "        edges, old_edge_id = [], []\n",
    "        \n",
    "        edges.extend(seq_edges)\n",
    "\n",
    "        old_edge_id.extend(seq_old_edges_id)\n",
    "\n",
    "        if self.is_cuda:\n",
    "            old_edge_id = torch.LongTensor(old_edge_id).cuda()\n",
    "        else:\n",
    "            old_edge_id = torch.LongTensor(old_edge_id)\n",
    "\n",
    "        srcs, dsts = zip(*edges)\n",
    "        # adding edges to graph\n",
    "        sub_graph.add_edges(srcs, dsts)\n",
    "\n",
    "        try:\n",
    "            seq_edges_w = self.seq_edge_w(old_edge_id)\n",
    "        except RuntimeError:\n",
    "            print(old_edge_id)\n",
    "      \n",
    "        sub_graph.edata['w'] = seq_edges_w\n",
    "\n",
    "        return sub_graph\n",
    "\n",
    "    def forward(self, doc_ids, doc_lengths):\n",
    "        # create corresponding text-level graph for each sentence\n",
    "        sub_graphs = [self.seq_to_graph(doc, length) for doc, length in zip(doc_ids, doc_lengths)]\n",
    "\n",
    "        batch_graph = dgl.batch(sub_graphs)\n",
    "        batch_graph.update_all(\n",
    "            message_func = dgl.function.src_mul_edge('h', 'w', 'weighted_message'),\n",
    "            reduce_func= dgl.function.max('weighted_message', 'h')\n",
    "        )\n",
    "\n",
    "        h1 = dgl.sum_nodes(batch_graph, feat='h')\n",
    "        drop1 = self.dropout(h1)\n",
    "        act1 = self.activation(drop1)\n",
    "        return act1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RwzqhR-nJ9wQ"
   },
   "outputs": [],
   "source": [
    "class BertFeature(transformers.BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.bert = transformers.BertModel(config)\n",
    "        self.init_weights()\n",
    "        self.features = config.hidden_size\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "    ):\n",
    "\n",
    "            bert_outputs = self.bert(\n",
    "              input_ids,\n",
    "              attention_mask=attention_mask,\n",
    "              token_type_ids=token_type_ids,\n",
    "              position_ids=position_ids,\n",
    "              head_mask=head_mask,\n",
    "              inputs_embeds=inputs_embeds,\n",
    "            )\n",
    "            pooled_output = bert_outputs[1]\n",
    "\n",
    "            return pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pknd5BguKA5R"
   },
   "outputs": [],
   "source": [
    "class BertGCN(torch.nn.Module):\n",
    "  def __init__(self, hidden, vocabulary, n_grams, edges_mappings, edges_weights, count, num_classes, dropout_prob):\n",
    "    super(BertGCN, self).__init__()\n",
    "    self.bert = BertFeature.from_pretrained(config.MODEL, output_attentions=False)\n",
    "    self.gcn = GCNModel(hidden_size_node = hidden,\n",
    "                            vocab = vocabulary,\n",
    "                            n_gram = n_grams,\n",
    "                            edges_matrix = edges_mappings,\n",
    "                            edges_num = count,\n",
    "                            trainable_edges = True, \n",
    "                            pmi = edges_weights, \n",
    "                            cuda = True\n",
    "                            )\n",
    "    self.num_classes = num_classes\n",
    "\n",
    "    self.dropout = nn.Dropout(dropout_prob)\n",
    "    self.bn1 = nn.BatchNorm1d(self.bert.features)\n",
    "    self.bn2 = nn.BatchNorm1d(hidden)\n",
    "    self.classifier = nn.Linear(self.bert.features + hidden, self.num_classes)\n",
    "\n",
    "  def forward(self, input_ids, attention_mask, input_gcn, sent_len, labels=None):\n",
    "    bert_out = self.bert(input_ids, attention_mask)\n",
    "    gcn_out = self.gcn(input_gcn, sent_len)\n",
    "\n",
    "    out = torch.cat((bert_out, gcn_out), 1)\n",
    "    logits = self.classifier(out)\n",
    "\n",
    "    outputs = (logits,)\n",
    "\n",
    "    if labels is not None:\n",
    "        if self.num_classes == 1:\n",
    "            loss_fct = MSELoss()\n",
    "            loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "        else:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_classes), labels.view(-1))\n",
    "        outputs = (loss,) + outputs\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9NGJrNcSx33W"
   },
   "outputs": [],
   "source": [
    "def train_fn(data_loader, model, optimizer, device):\n",
    "  model.train()\n",
    "  losses = AverageMeter()\n",
    "  tk0 = tqdm(data_loader, total=len(data_loader))\n",
    "  \n",
    "  for bi, d in enumerate(tk0):\n",
    "    ids = d['ids']\n",
    "    mask = d['mask']\n",
    "    gcn_ids = d['gcn_ids']\n",
    "    gcn_length = d['gcn_length']\n",
    "    label = d['label']\n",
    "\n",
    "    ids = ids.to(device, dtype=torch.long)\n",
    "    mask = mask.to(device, dtype=torch.long)\n",
    "    gcn_ids = gcn_ids.to(device, dtype=torch.long)\n",
    "    gcn_length = gcn_length.to(device, dtype=torch.long)\n",
    "    label = label.to(device, dtype=torch.long)\n",
    "    \n",
    "\n",
    "    model.zero_grad()\n",
    "    outputs = model(ids, mask, gcn_ids, gcn_length, label)\n",
    "\n",
    "    loss, logits = outputs[:2]\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    losses.update(loss.item(), ids.size(0))\n",
    "    tk0.set_postfix(loss=losses.avg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pSQNnIl-x3om"
   },
   "outputs": [],
   "source": [
    "def eval_fn(data_loader, model, device):\n",
    "  model.eval()\n",
    "  losses = AverageMeter()\n",
    "  tk0 = tqdm(data_loader, total=len(data_loader))\n",
    "  yt, yp = [], []\n",
    "\n",
    "  for bi, d in enumerate(tk0):\n",
    "    ids = d['ids']\n",
    "    mask = d['mask']\n",
    "    gcn_ids = d['gcn_ids']\n",
    "    gcn_length = d['gcn_length']\n",
    "    label = d['label']\n",
    "\n",
    "    ids = ids.to(device, dtype=torch.long)\n",
    "    mask = mask.to(device, dtype=torch.long)\n",
    "    gcn_ids = gcn_ids.to(device, dtype=torch.long)\n",
    "    gcn_length = gcn_length.to(device, dtype=torch.long)\n",
    "    label = label.to(device, dtype=torch.long)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "      outputs = model(ids, mask, gcn_ids, gcn_length, label)        \n",
    "      loss, logits = outputs[:2]\n",
    "\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "\n",
    "    preds = softmax(logits)\n",
    "    pred_labels = np.argmax(preds, axis=1).flatten()\n",
    "    ground_labels = label.to('cpu').numpy()\n",
    "\n",
    "    yt = yt + ground_labels.tolist()\n",
    "    yp = yp + pred_labels.tolist()\n",
    "\n",
    "    losses.update(loss.item(), ids.size(0))\n",
    "    tk0.set_postfix(loss=losses.avg)\n",
    "\n",
    "\n",
    "  print('Classification Report')\n",
    "  print(classification_report(yt, yp))   \n",
    "  # return losses.avg \n",
    "  return f1_score(yt, yp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RYE8XhTY6FzY"
   },
   "outputs": [],
   "source": [
    "def test_fn(data_loader, model, device):\n",
    "  model.eval()\n",
    "  tk0 = tqdm(data_loader, total=len(data_loader))\n",
    "  test_preds = []\n",
    "\n",
    "  for bi, d in enumerate(tk0):\n",
    "    ids = d['ids']\n",
    "    mask = d['mask']\n",
    "    gcn_ids = d['gcn_ids']\n",
    "    gcn_length = d['gcn_length']\n",
    "    \n",
    "    ids = ids.to(device, dtype=torch.long)\n",
    "    mask = mask.to(device, dtype=torch.long)\n",
    "\n",
    "    gcn_ids = gcn_ids.to(device, dtype=torch.long)\n",
    "    gcn_length = gcn_length.to(device, dtype=torch.long)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "      outputs = model(ids, mask, gcn_ids, gcn_length)        \n",
    "              \n",
    "    logits = outputs[0]\n",
    "\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    preds = softmax(logits)[:, 1]        \n",
    "    test_preds = test_preds + preds.tolist()\n",
    "\n",
    "  return test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FsFjxFeeVAjj"
   },
   "outputs": [],
   "source": [
    "def run(df_train, df_val, df_test, fold=None):\n",
    "\n",
    "  train_dataset = DEFTDataset(\n",
    "        text = df_train.Sentence.values,\n",
    "        subject = df_train.Subject.values,\n",
    "        label = df_train.Label.values,\n",
    "    )\n",
    "  \n",
    "  valid_dataset = DEFTDataset(\n",
    "        text = df_val.Sentence.values,\n",
    "        subject = df_val.Subject.values,\n",
    "        label = df_val.Label.values,\n",
    "    )\n",
    "  \n",
    "  test_dataset = DEFTDataset(\n",
    "        text = df_test.Sentence.values,\n",
    "        subject = df_test.Subject.values,\n",
    "        label = df_test.Label.values,\n",
    "    )\n",
    "  \n",
    "\n",
    "  train_data_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.TRAIN_BATCH_SIZE,\n",
    "        num_workers=4\n",
    "    )\n",
    "\n",
    "  valid_data_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=config.VALID_BATCH_SIZE,\n",
    "        num_workers=2\n",
    "    )\n",
    "  \n",
    "  test_data_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=config.VALID_BATCH_SIZE,\n",
    "        num_workers=2\n",
    "    )\n",
    "\n",
    "\n",
    "  edges_weights, edges_mappings, count = cal_PMI(train_dataset, 5)\n",
    "\n",
    "  model = BertGCN(hidden = 200,\n",
    "            vocabulary = train_dataset.vocab,\n",
    "            n_grams = 5,\n",
    "            edges_mappings = edges_mappings,\n",
    "            edges_weights = edges_weights, \n",
    "            count = count, \n",
    "            num_classes = 2, \n",
    "            dropout_prob = 0.3\n",
    "          )\n",
    "\n",
    "\n",
    "  device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n",
    "  model.to(device)\n",
    "\n",
    "  lr = 2e-5\n",
    "  param_optimizer = list(model.named_parameters())\n",
    "  no_decay = ['bias', 'gamma', 'beta']\n",
    "  optimizer_grouped_parameters = [\n",
    "      {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "      'weight_decay_rate': 0.01},\n",
    "      {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "      'weight_decay_rate': 0.0}\n",
    "  ]\n",
    "  optimizer = AdamW(optimizer_grouped_parameters, lr=lr)\n",
    "\n",
    "  es = EarlyStopping(patience=3, mode=\"max\")\n",
    "\n",
    "  print('Starting training....')\n",
    "  for epoch in range(config.EPOCHS):\n",
    "    train_fn(train_data_loader, model, optimizer, device)\n",
    "    valid_loss = eval_fn(valid_data_loader, model, device)\n",
    "    print(f'Epoch :{epoch + 1} | Validation Score :{valid_loss}')\n",
    "    if fold is None:\n",
    "      es(valid_loss, model, model_path=os.path.join(config.SAVE_DIR, f\"model.bin\"))\n",
    "    else:\n",
    "      es(valid_loss, model, model_path=os.path.join(config.SAVE_DIR, f\"model_{fold}.bin\"))\n",
    "\n",
    "\n",
    "  print('Predicting for test-set')\n",
    "  if fold is None:\n",
    "    model.load_state_dict(torch.load(os.path.join(config.SAVE_DIR, 'model.bin')))\n",
    "  else:\n",
    "    model.load_state_dict(torch.load(os.path.join(config.SAVE_DIR, f'model_{fold}.bin')))\n",
    "  model.to(device)\n",
    "  \n",
    "  test_predictions = test_fn(test_data_loader, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J--n0cA8W46y"
   },
   "outputs": [],
   "source": [
    "def run_train_val():\n",
    "  '''\n",
    "  Train model, validate and return predictions on test-set\n",
    "  '''\n",
    "  seed_all()\n",
    "  df_train = pd.read_csv(config.TRAIN_FILE)\n",
    "  df_val = pd.read_csv(config.VAL_FILE)\n",
    "  df_test = pd.read_csv(config.TEST_FILE)\n",
    "\n",
    "  df_test['Label'] = -1\n",
    "\n",
    "  scores = pd.DataFrame()\n",
    "  y = run(df_train, df_val, df_test)\n",
    "  scores['prob'] = y\n",
    "  \n",
    "  scores.to_csv(os.path.join(config.SAVE_DIR, 'submission.csv'), index=False)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sBzWLTb2X4pJ"
   },
   "outputs": [],
   "source": [
    "def run_k_fold():\n",
    "  '''\n",
    "    Perform k-fold cross-validation\n",
    "  '''\n",
    "\n",
    "  seed_all()\n",
    "  scores = pd.DataFrame()\n",
    "\n",
    "  df_train = pd.read_csv(config.TRAIN_FILE)\n",
    "  df_val = pd.read_csv(config.VAL_FILE)\n",
    "  df_test = pd.read_csv(config.TEST_FILE)\n",
    "\n",
    "  # concatenating train and validation set\n",
    "  train = pd.concat([df_train, df_val]).reset_index()\n",
    "  \n",
    "  # dividing folds\n",
    "  kf = model_selection.StratifiedKFold(n_splits=config.KFOLD, shuffle=False, random_state=config.SEED)\n",
    "  for fold, (train_idx, val_idx) in enumerate(kf.split(X=train, y=train.Label.values)):\n",
    "      train.loc[val_idx, 'kfold'] = fold\n",
    "\n",
    "  \n",
    "  df_test['Label'] = -1\n",
    "\n",
    "  for i in range(config.KFOLD):\n",
    "    print(f'################# Fold {i} #################')\n",
    "    df_train = train[train.kfold!=i]\n",
    "    df_val = train[train.kfold==i]\n",
    "\n",
    "    y = run(df_train, df_val, df_test, i)\n",
    "    scores[f'prob_{i}'] = y\n",
    "  \n",
    "  scores.to_csv(os.path.join(config.SAVE_DIR, 'submission.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cPdzeIXP1wUt"
   },
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    ! rm -rf {config.SAVE_DIR} && mkdir {config.SAVE_DIR}\n",
    "    # run_train_val()\n",
    "    run_k_fold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TM6MK2YoBiZ-"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BERT_GCN_joint_model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
